{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Property Scraper - Data Analysis\n",
    "\n",
    "This notebook analyzes scraped property listing data that's automatically committed to the repository by GitHub Actions.\n",
    "\n",
    "**Features:**\n",
    "- Loads CSV data from the `data/` directory\n",
    "- Provides data analysis and visualizations\n",
    "- Works in both local Jupyter and Google Colab\n",
    "- No GitHub CLI setup needed!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Check if running in Colab\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "print(f\"Running in: {'Google Colab' if IN_COLAB else 'Local Jupyter'}\")\n",
    "\n",
    "# Install required packages\n",
    "if IN_COLAB:\n",
    "    print(\"Installing packages...\")\n",
    "    !pip install -q pandas matplotlib seaborn\n",
    "else:\n",
    "    print(\"Packages should already be installed locally\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"✓ Packages imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Clone Repository (Colab Only)\n",
    "\n",
    "If you're in Colab, clone the repository to access the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    REPO_URL = \"https://github.com/yourusername/scraping_listings.git\"  # Change this!\n",
    "    \n",
    "    # Clone the repository\n",
    "    !git clone {REPO_URL}\n",
    "    \n",
    "    # Change to repo directory\n",
    "    os.chdir(\"scraping_listings\")\n",
    "    print(f\"✓ Repository cloned, working directory: {os.getcwd()}\")\n",
    "else:\n",
    "    print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Data from Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data directory\n",
    "DATA_DIR = Path(\"data\")\n",
    "\n",
    "if not DATA_DIR.exists():\n",
    "    print(f\"❌ Data directory not found: {DATA_DIR.absolute()}\")\n",
    "    print(\"Make sure you're in the correct directory!\")\n",
    "else:\n",
    "    print(f\"✓ Data directory found: {DATA_DIR.absolute()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_csv_files():\n",
    "    \"\"\"Find all CSV files in data directory.\"\"\"\n",
    "    csv_files = list(DATA_DIR.glob(\"*.csv\"))\n",
    "    \n",
    "    # Organize by type\n",
    "    pages_files = [f for f in csv_files if \"pages\" in f.name]\n",
    "    details_files = [f for f in csv_files if \"details\" in f.name]\n",
    "    \n",
    "    return {\n",
    "        'all': csv_files,\n",
    "        'pages': pages_files,\n",
    "        'details': details_files\n",
    "    }\n",
    "\n",
    "csv_files = find_all_csv_files()\n",
    "\n",
    "print(f\"Found CSV files:\")\n",
    "print(f\"  Total: {len(csv_files['all'])}\")\n",
    "print(f\"  Pages: {len(csv_files['pages'])}\")\n",
    "print(f\"  Details: {len(csv_files['details'])}\")\n",
    "\n",
    "if csv_files['all']:\n",
    "    print(f\"\\nSample files:\")\n",
    "    for f in list(csv_files['all'])[:5]:\n",
    "        print(f\"  - {f.name}\")\n",
    "else:\n",
    "    print(\"\\n⚠️ No CSV files found. The scrapers haven't run yet or the repo is empty.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load and Merge Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_merge_csvs(file_list, scraper_name=None):\n",
    "    \"\"\"Load and merge multiple CSV files into a single DataFrame.\"\"\"\n",
    "    if scraper_name:\n",
    "        file_list = [f for f in file_list if scraper_name in f.name]\n",
    "    \n",
    "    if not file_list:\n",
    "        print(f\"No files found for scraper: {scraper_name}\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"Loading {len(file_list)} files...\")\n",
    "    \n",
    "    dfs = []\n",
    "    for f in file_list:\n",
    "        try:\n",
    "            df = pd.read_csv(f)\n",
    "            dfs.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {f.name}: {e}\")\n",
    "    \n",
    "    if not dfs:\n",
    "        return None\n",
    "    \n",
    "    # Merge all DataFrames\n",
    "    merged_df = pd.concat(dfs, ignore_index=True)\n",
    "    \n",
    "    # Remove duplicates based on URL\n",
    "    if 'url' in merged_df.columns:\n",
    "        before = len(merged_df)\n",
    "        merged_df = merged_df.drop_duplicates(subset=['url'], keep='last')\n",
    "        after = len(merged_df)\n",
    "        if before != after:\n",
    "            print(f\"  Removed {before - after} duplicates\")\n",
    "    \n",
    "    # Convert date_time to datetime\n",
    "    if 'date_time' in merged_df.columns:\n",
    "        merged_df['date_time'] = pd.to_datetime(merged_df['date_time'])\n",
    "    \n",
    "    print(f\"✓ Loaded {len(merged_df)} records\")\n",
    "    return merged_df\n",
    "\n",
    "# Load data for each scraper\n",
    "scrapers = ['fincaraiz', 'metrocuadrado', 'mercado-libre']\n",
    "data = {}\n",
    "\n",
    "for scraper in scrapers:\n",
    "    print(f\"\\n--- Loading {scraper} data ---\")\n",
    "    \n",
    "    pages_df = load_and_merge_csvs(csv_files['pages'], scraper)\n",
    "    details_df = load_and_merge_csvs(csv_files['details'], scraper)\n",
    "    \n",
    "    data[scraper] = {\n",
    "        'pages': pages_df,\n",
    "        'details': details_df\n",
    "    }\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Data loading complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_data_summary():\n",
    "    \"\"\"Display summary of all loaded data.\"\"\"\n",
    "    summary_data = []\n",
    "    \n",
    "    for scraper, dfs in data.items():\n",
    "        pages_count = len(dfs['pages']) if dfs['pages'] is not None else 0\n",
    "        details_count = len(dfs['details']) if dfs['details'] is not None else 0\n",
    "        \n",
    "        summary_data.append({\n",
    "            'Scraper': scraper.title(),\n",
    "            'Pages Records': pages_count,\n",
    "            'Detail Records': details_count,\n",
    "            'Total': pages_count + details_count\n",
    "        })\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    display(summary_df)\n",
    "    \n",
    "    # Total\n",
    "    total_records = summary_df['Total'].sum()\n",
    "    print(f\"\\nTotal records across all scrapers: {total_records:,}\")\n",
    "\n",
    "show_data_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Detailed Analysis\n",
    "\n",
    "### Choose a scraper to analyze:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select scraper to analyze\n",
    "SELECTED_SCRAPER = 'fincaraiz'  # Change to 'metrocuadrado' or 'mercado-libre'\n",
    "\n",
    "df_details = data[SELECTED_SCRAPER]['details']\n",
    "\n",
    "if df_details is None or len(df_details) == 0:\n",
    "    print(f\"No data available for {SELECTED_SCRAPER}\")\n",
    "else:\n",
    "    print(f\"Analyzing {SELECTED_SCRAPER} data...\")\n",
    "    print(f\"Total records: {len(df_details):,}\")\n",
    "    print(f\"\\nColumns: {list(df_details.columns)}\")\n",
    "    print(f\"\\nFirst few records:\")\n",
    "    display(df_details.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the 'information' JSON column\n",
    "if df_details is not None and 'information' in df_details.columns:\n",
    "    print(\"Parsing information column...\")\n",
    "    \n",
    "    # Parse JSON strings\n",
    "    import ast\n",
    "    \n",
    "    def safe_parse(x):\n",
    "        try:\n",
    "            if isinstance(x, str):\n",
    "                return ast.literal_eval(x)\n",
    "            return x\n",
    "        except:\n",
    "            return {}\n",
    "    \n",
    "    df_details['info_parsed'] = df_details['information'].apply(safe_parse)\n",
    "    \n",
    "    # Extract common fields\n",
    "    if len(df_details) > 0:\n",
    "        sample_info = df_details['info_parsed'].iloc[0]\n",
    "        print(f\"\\nAvailable fields in information:\")\n",
    "        for key in sample_info.keys():\n",
    "            print(f\"  - {key}\")\n",
    "        \n",
    "        # Extract some fields as columns\n",
    "        for field in ['title', 'pricing', 'price', 'location', 'bedrooms', 'bathrooms', 'area']:\n",
    "            if field in sample_info:\n",
    "                df_details[field] = df_details['info_parsed'].apply(\n",
    "                    lambda x: x.get(field, None) if isinstance(x, dict) else None\n",
    "                )\n",
    "        \n",
    "        print(\"\\n✓ Information parsed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df_details is not None and len(df_details) > 0:\n",
    "    print(\"Data Statistics:\\n\")\n",
    "    \n",
    "    # Show scraped dates\n",
    "    if 'date_time' in df_details.columns:\n",
    "        print(f\"Date range: {df_details['date_time'].min()} to {df_details['date_time'].max()}\")\n",
    "        print(f\"Scraping sessions: {df_details['date_time'].dt.date.nunique()}\")\n",
    "    \n",
    "    # Show location distribution\n",
    "    if 'location' in df_details.columns:\n",
    "        print(f\"\\nTop 10 Locations:\")\n",
    "        location_counts = df_details['location'].value_counts().head(10)\n",
    "        display(location_counts)\n",
    "    \n",
    "    # Show price statistics\n",
    "    if 'pricing' in df_details.columns or 'price' in df_details.columns:\n",
    "        price_col = 'pricing' if 'pricing' in df_details.columns else 'price'\n",
    "        print(f\"\\nPrice Statistics:\")\n",
    "        \n",
    "        # Extract numeric prices\n",
    "        df_details['price_numeric'] = df_details[price_col].astype(str).str.extract(r'([\\d,.]+)')[0]\n",
    "        df_details['price_numeric'] = df_details['price_numeric'].str.replace(',', '').str.replace('.', '')\n",
    "        df_details['price_numeric'] = pd.to_numeric(df_details['price_numeric'], errors='coerce')\n",
    "        \n",
    "        valid_prices = df_details['price_numeric'].dropna()\n",
    "        if len(valid_prices) > 0:\n",
    "            print(f\"  Count: {len(valid_prices):,}\")\n",
    "            print(f\"  Mean: ${valid_prices.mean():,.0f}\")\n",
    "            print(f\"  Median: ${valid_prices.median():,.0f}\")\n",
    "            print(f\"  Min: ${valid_prices.min():,.0f}\")\n",
    "            print(f\"  Max: ${valid_prices.max():,.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df_details is not None and len(df_details) > 0:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle(f'{SELECTED_SCRAPER.title()} - Data Analysis', fontsize=16)\n",
    "    \n",
    "    # 1. Scraping timeline\n",
    "    if 'date_time' in df_details.columns:\n",
    "        df_details['date'] = df_details['date_time'].dt.date\n",
    "        daily_counts = df_details['date'].value_counts().sort_index()\n",
    "        axes[0, 0].plot(daily_counts.index, daily_counts.values, marker='o')\n",
    "        axes[0, 0].set_title('Records Scraped Over Time')\n",
    "        axes[0, 0].set_xlabel('Date')\n",
    "        axes[0, 0].set_ylabel('Number of Records')\n",
    "        axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # 2. Price distribution\n",
    "    if 'price_numeric' in df_details.columns:\n",
    "        valid_prices = df_details['price_numeric'].dropna()\n",
    "        if len(valid_prices) > 0:\n",
    "            # Remove outliers for better visualization\n",
    "            q1, q3 = valid_prices.quantile([0.25, 0.75])\n",
    "            iqr = q3 - q1\n",
    "            filtered_prices = valid_prices[(valid_prices >= q1 - 1.5*iqr) & (valid_prices <= q3 + 1.5*iqr)]\n",
    "            \n",
    "            axes[0, 1].hist(filtered_prices, bins=30, edgecolor='black')\n",
    "            axes[0, 1].set_title('Price Distribution (outliers removed)')\n",
    "            axes[0, 1].set_xlabel('Price')\n",
    "            axes[0, 1].set_ylabel('Frequency')\n",
    "    \n",
    "    # 3. Top locations\n",
    "    if 'location' in df_details.columns:\n",
    "        top_locations = df_details['location'].value_counts().head(10)\n",
    "        axes[1, 0].barh(range(len(top_locations)), top_locations.values)\n",
    "        axes[1, 0].set_yticks(range(len(top_locations)))\n",
    "        axes[1, 0].set_yticklabels(top_locations.index)\n",
    "        axes[1, 0].set_title('Top 10 Locations')\n",
    "        axes[1, 0].set_xlabel('Number of Listings')\n",
    "    \n",
    "    # 4. Bedrooms distribution\n",
    "    if 'bedrooms' in df_details.columns:\n",
    "        bedroom_counts = df_details['bedrooms'].value_counts().sort_index().head(10)\n",
    "        axes[1, 1].bar(bedroom_counts.index.astype(str), bedroom_counts.values)\n",
    "        axes[1, 1].set_title('Bedrooms Distribution')\n",
    "        axes[1, 1].set_xlabel('Number of Bedrooms')\n",
    "        axes[1, 1].set_ylabel('Count')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Export Merged Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export merged data to CSV\n",
    "output_dir = Path(\"merged_data\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "for scraper, dfs in data.items():\n",
    "    if dfs['details'] is not None and len(dfs['details']) > 0:\n",
    "        output_file = output_dir / f\"{scraper}_merged_details.csv\"\n",
    "        dfs['details'].to_csv(output_file, index=False)\n",
    "        print(f\"✓ Exported {scraper} data to {output_file}\")\n",
    "        print(f\"  Records: {len(dfs['details']):,}\")\n",
    "\n",
    "print(f\"\\nAll merged data saved to: {output_dir.absolute()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Download Merged Data (Colab Only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    from google.colab import files\n",
    "    import zipfile\n",
    "    \n",
    "    # Create ZIP of merged data\n",
    "    zip_filename = \"merged_property_data.zip\"\n",
    "    \n",
    "    with zipfile.ZipFile(zip_filename, 'w') as zipf:\n",
    "        for csv_file in output_dir.glob(\"*.csv\"):\n",
    "            zipf.write(csv_file, csv_file.name)\n",
    "    \n",
    "    print(f\"Downloading {zip_filename}...\")\n",
    "    files.download(zip_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Custom Analysis\n",
    "\n",
    "Use this cell for your own custom analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your custom analysis here\n",
    "# Example: Filter by location\n",
    "if df_details is not None:\n",
    "    # Example filter\n",
    "    # filtered_df = df_details[df_details['location'].str.contains('Chapinero', na=False)]\n",
    "    # display(filtered_df[['url', 'title', 'pricing', 'location']].head(10))\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
